<!-- Abstract -->
<body>
  <div class="text" style="font-size:50px">
    MultiVerse: Disentangled Modeling for Multi-Task Text-to-Speech
  </div>
  <div class="text" style="font-size:20px">
    Anonymous Authors
  </div>
  <div class="text" style="font-size:10px">
    &nbsp;
  </div>
  <div class="text" style="font-size:40px">
    Abstract
  </div>
  <div class="text" style="font-size:15px">
    This paper introduces "MultiVerse," a novel deep-learning model for multi-task speech synthesis. Addressing the challenges of generalization, we employ a disentangled modeling for speech that separates content, style, and prosody. To this end, our multi-task TTS model utilizes a source-filter model for feature disentanglement and prompt-based auto-regressive prosody modeling. The proposed model excels in zero-shot synthesis, cross-lingual synthesis, and style transfer. Additionally, it handles these tasks with a unified approach, consolidating them within a single, versatile framework. Leveraging disentangled modeling, MultiVerse achieves robust generalization, requiring relatively less training data compared to data-driven approaches. Experimental results demonstrate its remarkable zero-shot synthesis, even in cross-lingual scenarios, producing enhanced speech intelligibility, speaker similarity, and prosody similarity.
  </div>
  <p style="text-align: center;">
    <img src="./ditto.png" alt="Overview" width="800">
  </p>
</body>
